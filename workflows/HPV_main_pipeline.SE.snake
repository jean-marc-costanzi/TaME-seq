#to use workflow run
# $ source activate python3env

__author__="sium"
__email__="sinan.ugur.umu@kreftregisteret.no"


#required params: identifier
# vim: tabstop=8 expandtab shiftwidth=4 softtabstop=4


import glob
from collections import defaultdict
directories, files, = glob_wildcards("data/raw/{group}/{sample}.fastq.gz")

#files=list(filter(lambda x: x.find("_R1_001") == -1 and x.find("_R2_001") == -1,files))

#glob_wildcards("WGS/tests/fastq/{group}/{sample}.fastq.gzadr_R1.fastq")


#glob.glob

#print (directories)
#print (files)
#print (config)


file_dict=defaultdict(list)
#assign files to correct directories
for i in zip(directories,files):
	if i[1].find("_R1_001") == -1 and i[1].find("_R2_001") == -1:
        	file_dict[i[0]].append(i[1])

identifier=config.get("identifier","hg38-HPV183") #if it is empty, use the default

#which reference to use
#
reference_base="/WORKING/databases/HPV/HPV_hg38/"

if identifier.find("shift") >= 0 :
	reference=reference_base + "hg38_HPV183refseq_shifted"
elif identifier.find("added") >= 0 :
	reference=reference_base + "hg38_HPV183refseq_added"
else:
	reference=reference_base + "hg38_HPV183refseq"


#other possibilities: h38-HPV183-added hg38-HPV183-shifted



#extensions=[".collapsed.bed"]
#sub_directories=["softclip_files/"]


command_base="scripts/"
directory_base="analyses/"


rule all:
	input:
		#[expand(directory_base + extension[0] + "{group}/{sample}." + identifier + extension[1] ,zip,group=directories,sample=files) for extension in zip(sub_directories,extensions)],
		expand("results/{group}/{group}.fastq_stats.csv",group=set(file_dict.keys())),
		expand("results/{group}/{group}." + identifier + ".stats.csv",group=set(file_dict.keys())),
		expand("results/{group}/{group}." + identifier + ".coverage.csv",group=set(file_dict.keys())),
                #expand("results/{group}/{group}." + identifier + ".discordant.coverage.csv",group=set(file_dict.keys())),
		#expand("results/{group}/{group}." + identifier + ".discordant.reads.csv",group=set(file_dict.keys())),
		#expand("results/{group}/{group}." + identifier + ".soft-clipped.csv",group=set(file_dict.keys())),
		expand("results/{group}/{group}." + identifier + ".lastal.reads.csv",group=set(file_dict.keys()))	

#auto reduce to 1000000 reads per sample
rule downsampling_fastq:
	input:
		"data/raw/{group}/{sample}.fastq.gz"
	output:
		temp("data/raw/{group}/{sample}.fastq")
	params:
		count="10000000"
	run:
		shell("seqtk sample -s255 {input} {params.count} > {output}")


###trimming adapters
rule adapter_trimming:
	input:
		"data/raw/{group}/{sample}.fastq"
	output:
                "data/trimmed/{group}/{sample}.trimmed.fastq.gz"
	run:
		shell("cutadapt -O 8 -a file:/WORKING/databases/HPV/primersHPV/HPV-WGS_primers_adapters_revcomp.fasta -q 20 -m 50 -o {output} {input}")

###create fastq file statistics before and after trimming
rule fastq_stats:
	input:
		"data/raw/{group}/{sample}.fastq.gz",
                "data/trimmed/{group}/{sample}.trimmed.fastq.gz"

	output:
		"data/raw/{group}/{sample}.fastq_stats",
		"data/trimmed/{group}/{sample}.trimmed.fastq_stats",
		"data/fastq_stats/{group}/{sample}.fastq_stats.txt"

	run:
		for i,o in zip(input,output[0:2]):
			shell("fastq-stats {i} > {o}")

		shell("""
			raw_reads=$(cat {output[0]} | grep "^reads" | cut -f2);
			trimmed_reads=$(cat {output[1]} | grep "^reads" | cut -f2);
			raw_length=$(cat {output[0]} | grep "^len mean" | cut -f2);
			r1_trimmed_length=$(cat {output[1]} | grep "^len mean" | cut -f2);
			echo "{wildcards.sample}\t"$raw_reads"\t"$trimmed_reads"\t"$raw_length"\t"$r1_trimmed_length > {output[2]};
		""")

###map with Hisat2.
rule hisat2_mapping:
	input:
		"data/trimmed/{group}/{sample}.trimmed.fastq.gz"
	output:
		directory_base + "hisat2_bam_files/{group}/{sample}.hisat2." + identifier + ".sorted.bam",
		directory_base + "hisat2_bam_files/{group}/{sample}.hisat2." + identifier + ".sorted.bam.bai"

	threads: 15

	params:
		genome=reference

	run:
		shell("hisat2 -p {threads} --pen-noncansplice 0 -x {params.genome} -U {input} | samtools view -bS - | samtools sort - -o {output[0]}")
		shell("samtools index {output[0]}")


rule extract_unmapped_reads:
	input:
		directory_base + "hisat2_bam_files/{group}/{sample}.hisat2." + identifier + ".sorted.bam"

	output:
		directory_base + "hisat2_bam_files/{group}/{sample}." + identifier + ".unmapped.fasta"

	shell:
		"samtools view -b -f4 {input} | samtools fasta - > {output}"


rule lastal_unmapped_reads:
	input:
		directory_base + "hisat2_bam_files/{group}/{sample}." + identifier + ".unmapped.fasta"

	output:
		directory_base + "lastal/{group}/{sample}." + identifier + ".maf"

	threads: 15 

	params:
		lastdb=reference + "_lastaldb",
		fai=reference + ".fasta.fai"
	shell:
		"""
		lastal -P {threads} -M -C2 {params.lastdb} {input} > {output};
		"""

rule create_lastal_bams:
	input:
		directory_base + "hisat2_bam_files/{group}/{sample}." + identifier + ".unmapped.fasta",
		directory_base + "lastal/{group}/{sample}." + identifier + ".maf"
	output:
                directory_base + "lastal/{group}/{sample}."+ identifier + ".sorted.bam",
                directory_base + "lastal/{group}/{sample}." + identifier +".sorted.bam.bai"
	params:
		fai=reference + ".fasta.fai"
	shell:
		"""
		maf-convert psl {input[1]} | /WORKING/apps/miniconda2/envs/python2env/bin/uncle_psl.py -f {input[0]} | samtools view -bt {params.fai} - | samtools sort -o {output[0]};
                samtools index {output[0]};
		"""


###create coverage tables and discordant coverage tables
#these tables are created only for the HPV chromosome with the corresponding ID.
rule hpv_variant_and_discordant_calling:
	input:
		directory_base + "hisat2_bam_files/{group}/{sample}.hisat2." + identifier + ".sorted.bam"

	output:
		directory_base + "coverage_tables/{group}/{sample}." + identifier + ".coverage.csv"
		#directory_base + "coverage_tables/{group}/{sample}." + identifier + ".discordant.coverage.csv"
	
	params:
		ref=reference + ".fasta",
		awk="""awk -v sample_id=$sample_id '{if(NR==1) print $0"\tsample_id"; else print $0"\t"sample_id}'"""
	run:
		shell("""
			sample_id={wildcards.sample};
			{command_base}python/hpv-variant-call.py {input} {output} --auto --reference {params.ref};
			cat {output} | {params.awk} | sponge {output};
		 """)


###create FASTA file of soft clipped regions and associated bed files
#these are created for all chromosomes in the reference genome file so not necessarily from the HPV chromosomes
rule softclipping_bed_files:
	input:  
		directory_base + "hisat2_bam_files/{group}/{sample}.hisat2." + identifier + ".sorted.bam"

	output:
		directory_base + "softclip_files/{group}/{sample}." + identifier + ".soft-clipped.fasta",
		directory_base + "softclip_files/{group}/{sample}." + identifier + ".soft-clipped.bed"

	run:
		shell(command_base + "python/hpv-variant-call.py {input} {output[0]} {output[1]}")
		shell("sort -k1,1 -k2,2n {output[1]} | sponge {output[1]}")

###create discordant reads table

rule discordant_read_table:
	input:
		directory_base + "hisat2_bam_files/{group}/{sample}.hisat2." + identifier + ".sorted.bam"

	output:
		directory_base + "discordant_read_tables/{group}/{sample}." + identifier + ".discordant_reads.csv"

	run:
		shell("""
                sample_id={wildcards.sample};
		echo "sample_id\tread\tchr1\tr1_pos\tcigar1\tchr2\tr2_pos" > {output};
		samtools view {input} | grep "AS:" | awk '$7 !="="{{print}}' | awk -v sample_id=$sample_id '{{print sample_id"\t"$1"\t"$3"\t"$4"\t"$6"\t"$7"\t"$8}}' >> {output};
		""")

rule lastal_read_table:
	input:
		directory_base + "lastal/{group}/{sample}."+ identifier + ".sorted.bam"
	output:
		directory_base + "lastal_read_tables/{group}/{sample}." + identifier + ".lastal_reads.csv"
	run:
		shell("""
                sample_id={wildcards.sample};
                echo "sample_id\tread\tchr\tr_pos\tcigar" > {output};
                samtools view {input} | awk -v sample_id=$sample_id '{{print sample_id"\t"$1"\t"$3"\t"$4"\t"$6}}' >> {output};
                """)



###collapse bed files to get a short list of soft-clipping regions
rule collapse_bed_files:
	input:
		directory_base + "softclip_files/{group}/{sample}." + identifier + ".soft-clipped.bed"

	output:
		directory_base + "softclip_files/{group}/{sample}." + identifier + ".collapsed.bed"
	
	run:
		shell("bedtools merge -i {input} -c 4 -o count,distinct > {output}")
	
###remap soft-clipped sequences back to the reference genomes	
rule bowtie2_remap:
	input:
		directory_base + "softclip_files/{group}/{sample}." + identifier + ".soft-clipped.fasta"

	output: 
		directory_base + "bowtie2_remap/{group}/{sample}.bowtie2." + identifier + ".sorted.bam",
		directory_base + "bowtie2_remap/{group}/{sample}.bowtie2." + identifier + ".sorted.bam.bai"

	threads: 5
	
	params:
		genome=reference

	run:
		shell("bowtie2 -p {threads} -f -x {params.genome} -U {input} | samtools view -bS - | samtools sort - -o {output[0]}")
		shell("samtools index {output[0]}")

###create bed files of remapped reads
rule remapping_bed_files:
	input:
		directory_base + "bowtie2_remap/{group}/{sample}.bowtie2." + identifier + ".sorted.bam"
	output:
		directory_base + "bowtie2_remap/{group}/{sample}." + identifier + ".bed"
	run:
		shell("bamToBed -i {input} > {output}")


###merge those bed files of remapped reads
rule merge_bed_files:
	input:
		directory_base + "bowtie2_remap/{group}/{sample}." + identifier + ".bed",
		directory_base + "softclip_files/{group}/{sample}." + identifier + ".soft-clipped.bed"
	output:
		directory_base + "bowtie2_remap/{group}/{sample}." + identifier + ".merged.bed"

	run:
		
		shell("""
		sample_id={wildcards.sample};
		{command_base}R/merge-bed-files.R {input} | awk -v sample_id=$sample_id '{{print $0"\t"sample_id}}' > {output};

		""")
		


###create stats of hisat2 and bowtie2 mappings
rule hisat2_mapping_statistics:
	input:
		[directory_base + bam[0] + "/{group}/{sample}." + bam[1] + identifier + ".sorted.bam" for bam in zip(["hisat2_bam_files","bowtie2_remap"],["hisat2.","bowtie2."])]

	output:
		[directory_base + bam + "/{group}/{sample}." + identifier + ".statistics.txt" for bam in ["hisat2_bam_files","bowtie2_remap"]],
		[directory_base + bam + "/{group}/{sample}." + identifier + ".chromosome.txt" for bam in ["hisat2_bam_files","bowtie2_remap"]]

	params:
		awk="""awk -v file=${file/.sorted.bam/} -v strand=$strand -v mapper=$mapper -v sample_id=$sample_id 'BEGIN{match(file,/(HPV[0-9]+)/,m)}{print $0"\t"m[1]"\t"strand"\t"file"\t"mapper"\t"sample_id}'"""
		
	run:
		for i,f in zip([0,1],input):
			stat=output[i]
			chr=output[i+2]
			print (f)							
			shell("""
			mapper=$(echo {f} | awk '{{if(/bowtie2/) print "bowtie2"; else print "hisat2";}}');
			strand=$(echo {f} | awk '{{if(/F/) print "F"; else print "R";}}');
			file=$(basename {f});
			sample_id={wildcards.sample}
			samtools stats {f} | grep ^SN | cut -f 2- | {command_base}python/parse_samfile_stats.py | {params.awk} > {stat};
			{command_base}python/rnaseq_tool.py {f} | {params.awk} > {chr};
			""")

###combine fastq stats into a file
rule fastq_stats_combine:
	input:
		[lambda wildcards: ["data/" + z + "/" + wildcards.group + "/" + x + y for x in file_dict[wildcards.group]] for y in [".fastq_stats.txt"] for z in ["fastq_stats"]]

	output:
		"results/{group}/{group}.fastq_stats.csv"
        
	run:
		shell("""
		echo "sample_id\traw_reads\ttrimmed_reads\traw_length\tr1_trimmed_length\tr2_trimmed_length\t" > {output};
		cat {input} >> {output};
		""")


###combine all statistics into a file
rule combine_statistics:
        input:
                [lambda wildcards: [directory_base + z + "/" + wildcards.group + "/" + x + "." + identifier + y for x in file_dict[wildcards.group]] for y in [".statistics.txt",".chromosome.txt"] for z in ["hisat2_bam_files","bowtie2_remap"]]

        output:
                "results/{group}/{group}." + identifier + ".stats.csv",
                "results/{group}/{group}." + identifier + ".chromosome.csv"

        run:
                shell("cat " + directory_base + "*/{wildcards.group}/*" + identifier + """.statistics.txt | awk 'BEGIN{{print "stats\tvalue\tstrain\tstrand\tfile\tmapper\tsample_id"}}{{print}}' > {output[0]}""")
                shell("cat " + directory_base + "*/{wildcards.group}/*" + identifier + """.chromosome.txt | awk 'BEGIN{{print "chr\tcount\tstrain\tstrand\tfile\tmapper\tsample_id"}}{{print}}'  > {output[1]}""")



###combine coverage tables
rule combine_coverage_tables:
	input:
		lambda wildcards: [directory_base + "coverage_tables/" + wildcards.group + "/" + x + "." + identifier + y for x in file_dict[wildcards.group]] for y in [".coverage.csv"]
	output:
		"results/{group}/{group}." + identifier + ".coverage.csv"

	params:
		awk="""awk 'BEGIN{print "chr\tposition\treference\tcoverage\tA\tG\tC\tT\tdeletion\tskip\tqA\tqG\tqC\tqT\tsample_id"}{print $0}'"""
	run:
		shell("""
                        cat {directory_base}coverage_tables/{wildcards.group}/*{identifier}.coverage.csv | grep -v "position" | {params.awk} > {output};
                """)


rule combine_discordant_read_tables:
	input:
		lambda wildcards: [directory_base + "discordant_read_tables/" + wildcards.group + "/" + x + "." + identifier + ".discordant_reads.csv" for x in file_dict[wildcards.group]]
	output:
		"results/{group}/{group}." + identifier + ".discordant.reads.csv"
	run:
		shell("""
		echo "sample_id\tread\tchr1\tr1_pos\tcigar1\tchr2\tr2_pos" > {output};
		cat {input} | grep -v "sample_id" >> {output};
		""")


rule combine_lastal_read_tables:
	input:
		lambda wildcards: [directory_base + "lastal_read_tables/" + wildcards.group + "/" + x + "." + identifier + ".lastal_reads.csv" for x in file_dict[wildcards.group]]
	output:
		"results/{group}/{group}." + identifier + ".lastal.reads.csv"
	run:
		shell("""
                echo "sample_id\tread\tchr\tr_pos\tcigar" > {output};
                cat {input} | grep -v "sample_id" >> {output};
                """)


rule combine_merged_bed_files:
	input:
		lambda wildcards: [directory_base + "bowtie2_remap/" + wildcards.group + "/" + x + "." + identifier + ".merged.bed" for x in file_dict[wildcards.group]]
	output:
		"results/{group}/{group}." + identifier + ".soft-clipped.csv"
	run:
		shell("""
		echo "chr\tstart\tend\tname\tscore\tstrand\tsoftclipchr\tsoftclippos\tsample_id" > {output};
		cat {input} >> {output};
		""")
		


