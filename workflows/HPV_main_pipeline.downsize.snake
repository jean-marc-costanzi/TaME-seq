#to use workflow run
# $ source activate python3env

__author__="sium"
__email__="sinan.ugur.umu@kreftregisteret.no"


#required params: identifier
# vim: tabstop=8 expandtab shiftwidth=4 softtabstop=4


import glob
from collections import defaultdict
directories, files, = glob_wildcards("data/bam/{group}/{sample}.hisat2.hg38-HPV183.sorted.bam")
#glob_wildcards("WGS/tests/fastq/{group}/{sample}.fastq.gzadr_R1.fastq")


#glob.glob

#print (directories)
#print (files)
#print (config)


file_dict=defaultdict(list)
#assign files to correct directories
for i in zip(directories,files):
        file_dict[i[0]].append(i[1])



identifier=config.get("identifier","hg38-HPV183") #if it is empty, use the default
downsize=config.get("downsize",100000) #if it is empty, use the default

fraction=str(int(downsize/1000)) + "K"

identifier = identifier + "-d." + str(fraction)

#which reference to use
#
reference_base="/WORKING/databases/HPV/HPV_hg38/"

if identifier.find("shift") >= 0 :
	reference=reference_base + "hg38_HPV183refseq_shifted"
elif identifier.find("added") >= 0 :
	reference=reference_base + "hg38_HPV183refseq_added"
else:
	reference=reference_base + "hg38_HPV183refseq"


#other possibilities: h38-HPV183-added hg38-HPV183-shifted



#extensions=[".collapsed.bed"]
#sub_directories=["softclip_files/"]


command_base="scripts/"
directory_base="analyses/"


rule all:
	input:
		expand("results/{group}/{group}." + identifier + ".stats.csv",group=set(directories)),
		expand("results/{group}/{group}." + identifier + ".coverage.csv",group=set(directories)),
                expand("results/{group}/{group}." + identifier + ".discordant.coverage.csv",group=set(directories)),
		expand("results/{group}/{group}." + identifier + ".discordant.reads.csv",group=set(directories)),
		expand("results/{group}/{group}." + identifier + ".lastal.reads.csv",group=set(directories))	




rule hisat2_bam_downsampling:
	input:
		"data/bam/{group}/{sample}.hisat2.hg38-HPV183.sorted.bam"
	output:
		directory_base + "hisat2_bam_files/{group}/{sample}.hisat2." + identifier + ".sorted.bam",
                directory_base + "hisat2_bam_files/{group}/{sample}.hisat2." + identifier + ".sorted.bam.bai"
	
	run:
		shell("""
		#reads=$(samtools view -c {input});
		reads=$(samtools view {input} | grep -c HPV);
		fraction=$(awk -v reads=$reads -v downsize={downsize} 'BEGIN{{printf (downsize/reads)}}');
		#samtools view -s $fraction -b {input} > {output[0]};
		samtools view -h {input} | egrep "@SQ|HPV" | samtools view -h -s $fraction -b > {output[0]}; 
		samtools index {output[0]};
		""")
		

rule extract_unmapped_reads:
	input:
		directory_base + "hisat2_bam_files/{group}/{sample}.hisat2." + identifier + ".sorted.bam"

	output:
		directory_base + "hisat2_bam_files/{group}/{sample}." + identifier + ".unmapped.fasta"

	shell:
		"samtools view -b -f4 {input} | samtools fasta - > {output}"


rule lastal_unmapped_reads:
	input:
		directory_base + "hisat2_bam_files/{group}/{sample}." + identifier + ".unmapped.fasta"

	output:
		directory_base + "lastal/{group}/{sample}." + identifier + ".maf"

	threads: 15 

	params:
		lastdb=reference + "_lastaldb",
		fai=reference + ".fasta.fai"
	shell:
		"""
		lastal -P {threads} -M -C2 {params.lastdb} {input} > {output};
		"""

rule create_lastal_bams:
	input:
		directory_base + "hisat2_bam_files/{group}/{sample}." + identifier + ".unmapped.fasta",
		directory_base + "lastal/{group}/{sample}." + identifier + ".maf"
	output:
                directory_base + "lastal/{group}/{sample}."+ identifier + ".sorted.bam",
                directory_base + "lastal/{group}/{sample}." + identifier +".sorted.bam.bai"
	params:
		fai=reference + ".fasta.fai"
	shell:
		"""
		maf-convert psl {input[1]} | /WORKING/apps/miniconda2/envs/python2env/bin/uncle_psl.py -f {input[0]} | samtools view -bt {params.fai} - | samtools sort -o {output[0]};
                samtools index {output[0]};
		"""


###create coverage tables and discordant coverage tables
#these tables are created only for the HPV chromosome with the corresponding ID.
rule hpv_variant_and_discordant_calling:
	input:
		directory_base + "hisat2_bam_files/{group}/{sample}.hisat2." + identifier + ".sorted.bam"

	output:
		directory_base + "coverage_tables/{group}/{sample}." + identifier + ".coverage.csv",
		directory_base + "coverage_tables/{group}/{sample}." + identifier + ".discordant.coverage.csv"
	
	params:
		ref=reference + ".fasta",
		awk="""awk -v sample_id=$sample_id '{if(NR==1) print $0"\tsample_id"; else print $0"\t"sample_id}'"""
	run:
		shell("""
			sample_id={wildcards.sample};
			{command_base}python/hpv-variant-call.py {input} {output} --discordant --auto --reference {params.ref};
			cat {output[0]}  | {params.awk} | sponge {output[0]};
			cat {output[1]}  | {params.awk} | sponge {output[1]};
		""")


###create FASTA file of soft clipped regions and associated bed files
#these are created for all chromosomes in the reference genome file so not necessarily from the HPV chromosomes
rule softclipping_bed_files:
	input:  
		directory_base + "hisat2_bam_files/{group}/{sample}.hisat2." + identifier + ".sorted.bam"

	output:
		directory_base + "softclip_files/{group}/{sample}." + identifier + ".soft-clipped.fasta",
		directory_base + "softclip_files/{group}/{sample}." + identifier + ".soft-clipped.bed"

	run:
		shell(command_base + "python/hpv-variant-call.py {input} {output[0]} {output[1]}")
		shell("sort -k1,1 -k2,2n {output[1]} | sponge {output[1]}")

###create discordant reads table
rule discordant_read_table:
	input:
		directory_base + "hisat2_bam_files/{group}/{sample}.hisat2." + identifier + ".sorted.bam"

	output:
		directory_base + "discordant_read_tables/{group}/{sample}." + identifier + ".discordant_reads.csv"

	run:
		shell("""
                sample_id={wildcards.sample};
		echo "sample_id\tread\tchr1\tr1_pos\tcigar1\tchr2\tr2_pos" > {output};
		samtools view {input} | grep "AS:" | awk '$7 !="="{{print}}' | awk -v sample_id=$sample_id '{{print sample_id"\t"$1"\t"$3"\t"$4"\t"$6"\t"$7"\t"$8}}' >> {output};
		""")

rule lastal_read_table:
	input:
		directory_base + "lastal/{group}/{sample}."+ identifier + ".sorted.bam"
	output:
		directory_base + "lastal_read_tables/{group}/{sample}." + identifier + ".lastal_reads.csv"
	run:
		shell("""
                sample_id={wildcards.sample};
                echo "sample_id\tread\tchr\tr_pos\tcigar" > {output};
                samtools view {input} | awk -v sample_id=$sample_id '{{print sample_id"\t"$1"\t"$3"\t"$4"\t"$6}}' >> {output};
                """)



###collapse bed files to get a short list of soft-clipping regions
rule collapse_bed_files:
	input:
		directory_base + "softclip_files/{group}/{sample}." + identifier + ".soft-clipped.bed"

	output:
		directory_base + "softclip_files/{group}/{sample}." + identifier + ".collapsed.bed"
	
	run:
		shell("bedtools merge -i {input} -c 4 -o count,distinct > {output}")
	
###remap soft-clipped sequences back to the reference genomes	
rule bowtie2_remap:
	input:
		directory_base + "softclip_files/{group}/{sample}." + identifier + ".soft-clipped.fasta"

	output: 
		directory_base + "bowtie2_remap/{group}/{sample}.bowtie2." + identifier + ".sorted.bam",
		directory_base + "bowtie2_remap/{group}/{sample}.bowtie2." + identifier + ".sorted.bam.bai"

	threads: 5
	
	params:
		genome=reference

	run:
		shell("bowtie2 -p {threads} -f -x {params.genome} -U {input} | samtools view -bS - | samtools sort - -o {output[0]}")
		shell("samtools index {output[0]}")

###create bed files of remapped reads
rule remapping_bed_files:
	input:
		directory_base + "bowtie2_remap/{group}/{sample}.bowtie2." + identifier + ".sorted.bam"
	output:
		directory_base + "bowtie2_remap/{group}/{sample}." + identifier + ".bed"
	run:
		shell("bamToBed -i {input} > {output}")


###merge those bed files of remapped reads
rule merge_bed_files:
	input:
		directory_base + "bowtie2_remap/{group}/{sample}." + identifier + ".bed",
		directory_base + "softclip_files/{group}/{sample}." + identifier + ".soft-clipped.bed"
	output:
		directory_base + "bowtie2_remap/{group}/{sample}." + identifier + ".merged.bed"

	run:
		
		shell("""
		sample_id={wildcards.sample};
		{command_base}R/merge-bed-files.R {input} | awk -v sample_id=$sample_id '{{print $0"\t"sample_id}}' > {output};

		""")
		


###create stats of hisat2 and bowtie2 mappings
rule hisat2_mapping_statistics:
	input:
		[directory_base + bam[0] + "/{group}/{sample}." + bam[1] + identifier + ".sorted.bam" for bam in zip(["hisat2_bam_files","bowtie2_remap"],["hisat2.","bowtie2."])]

	output:
		[directory_base + bam + "/{group}/{sample}." + identifier + ".statistics.txt" for bam in ["hisat2_bam_files","bowtie2_remap"]],
		[directory_base + bam + "/{group}/{sample}." + identifier + ".chromosome.txt" for bam in ["hisat2_bam_files","bowtie2_remap"]]

	params:
		awk="""awk -v file=${file/.sorted.bam/} -v strand=$strand -v mapper=$mapper -v sample_id=$sample_id 'BEGIN{match(file,/(HPV[0-9]+)/,m)}{print $0"\t"m[1]"\t"strand"\t"file"\t"mapper"\t"sample_id}'"""
		
	run:
		for i,f in zip([0,1],input):
			stat=output[i]
			chr=output[i+2]
			print (f)							
			shell("""
			mapper=$(echo {f} | awk '{{if(/bowtie2/) print "bowtie2"; else print "hisat2";}}');
			strand=$(echo {f} | awk '{{if(/F/) print "F"; else print "R";}}');
			file=$(basename {f});
			sample_id={wildcards.sample}
			samtools stats {f} | grep ^SN | cut -f 2- | {command_base}python/parse_samfile_stats.py | {params.awk} > {stat};
			{command_base}python/rnaseq_tool.py {f} | {params.awk} > {chr};
			""")


###combine all statistics into a file
rule combine_statistics:
        input:
                [lambda wildcards: [directory_base + z + "/" + wildcards.group + "/" + x + "." + identifier + y for x in file_dict[wildcards.group]] for y in [".statistics.txt",".chromosome.txt"] for z in ["hisat2_bam_files","bowtie2_remap"]]

        output:
                "results/{group}/{group}." + identifier + ".stats.csv",
                "results/{group}/{group}." + identifier + ".chromosome.csv"

        run:
                shell("cat " + directory_base + "*/{wildcards.group}/*" + identifier + """.statistics.txt | awk 'BEGIN{{print "stats\tvalue\tstrain\tstrand\tfile\tmapper\tsample_id"}}{{print}}' > {output[0]}""")
                shell("cat " + directory_base + "*/{wildcards.group}/*" + identifier + """.chromosome.txt | awk 'BEGIN{{print "chr\tcount\tstrain\tstrand\tfile\tmapper\tsample_id"}}{{print}}'  > {output[1]}""")



###combine coverage tables
rule combine_coverage_tables:
	input:
		lambda wildcards: [directory_base + "coverage_tables/" + wildcards.group + "/" + x + "." + identifier + y for x in file_dict[wildcards.group]] for y in [".coverage.csv",".discordant.coverage.csv"]
	output:
		"results/{group}/{group}." + identifier + ".coverage.csv",
		"results/{group}/{group}." + identifier + ".discordant.coverage.csv"

	params:
		awk="""awk 'BEGIN{print "chr\tposition\treference\tcoverage\tA\tG\tC\tT\tdeletion\tskip\tqA\tqG\tqC\tqT\tsample_id"}{print $0}'"""
	run:
		shell("""
                        cat {directory_base}coverage_tables/{wildcards.group}/*{identifier}.coverage.csv | grep -v "position" | {params.awk} > {output[0]};
                        cat {directory_base}coverage_tables/{wildcards.group}/*{identifier}.discordant.coverage.csv | grep -v "position" | {params.awk} > {output[1]};
                """)


rule combine_discordant_read_tables:
	input:
		lambda wildcards: [directory_base + "discordant_read_tables/" + wildcards.group + "/" + x + "." + identifier + ".discordant_reads.csv" for x in file_dict[wildcards.group]]
	output:
		"results/{group}/{group}." + identifier + ".discordant.reads.csv"
	run:
		shell("""
		echo "sample_id\tread\tchr1\tr1_pos\tcigar1\tchr2\tr2_pos" > {output};
		cat {input} | grep -v "sample_id" >> {output};
		""")


rule combine_lastal_read_tables:
	input:
		lambda wildcards: [directory_base + "lastal_read_tables/" + wildcards.group + "/" + x + "." + identifier + ".lastal_reads.csv" for x in file_dict[wildcards.group]]
	output:
		"results/{group}/{group}." + identifier + ".lastal.reads.csv"
	run:
		shell("""
                echo "sample_id\tread\tchr\tr_pos\tcigar" > {output};
                cat {input} | grep -v "sample_id" >> {output};
                """)


rule combine_merged_bed_files:
	input:
		lambda wildcards: [directory_base + "bowtie2_remap/" + wildcards.group + "/" + x + "." + identifier + ".merged.bed" for x in file_dict[wildcards.group]]
	output:
		"results/{group}/{group}." + identifier + ".soft-clipped.csv"
	run:
		shell("""
		echo "chr\tstart\tend\tname\tscore\tstrand\tsoftclipchr\tsoftclippos\tsample_id" > {output};
		cat {input} >> {output};
		""")
		


